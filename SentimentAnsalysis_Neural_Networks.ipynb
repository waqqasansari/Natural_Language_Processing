{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnsalysis_Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPdXfTFDmXMlEpBOK9PW31p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqqasansari/Natural_Language_Processing/blob/master/SentimentAnsalysis_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5F0xkd4rtqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q -U trax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N84hgq4NmYW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "7e52984b-baeb-46d6-c038-219c9de01267"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.stem import PorterStemmer\n",
        "stopwords_english = stopwords.words('english')\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmHlBkH_r9jO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "021fa817-a308-42b2-9232-8fa02c18c58a"
      },
      "source": [
        "import trax\n",
        "import os \n",
        "import random as rnd\n",
        "import trax.fastmath.numpy as np\n",
        "from trax import layers as tl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVSJnuWxqoGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_tweets():\n",
        "  all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "  all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "  return all_positive_tweets, all_negative_tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkucMtx-nSGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_tweet(tweet):\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, \n",
        "                             strip_handles=True, \n",
        "                             reduce_len=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "  tweets_clean = []\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)\n",
        "      tweets_clean.append(stem_word)\n",
        "\n",
        "  return tweets_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ahjlVlrfYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "b68ea410-8216-40e0-efc2-0a504f0e3859"
      },
      "source": [
        "all_positive_tweets, all_negative_tweets = load_tweets()\n",
        "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
        "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
        "\n",
        "val_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "\n",
        "val_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "\n",
        "val_x = val_pos + val_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "val_y = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
        "\n",
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of positive tweets: 5000\n",
            "The number of negative tweets: 5000\n",
            "length of train_x 8000\n",
            "length of val_x 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9HAwtyxsf7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d0c63d1-2977-4e0c-cc0d-590c16c3bf33"
      },
      "source": [
        "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2}\n",
        "\n",
        "for tweet in train_x:\n",
        "  processed_tweet = process_tweet(tweet)\n",
        "  for word in processed_tweet:\n",
        "    if word not in Vocab:\n",
        "      Vocab[word] = len(Vocab)\n",
        "\n",
        "print(\"Total words in vocab are\",len(Vocab))\n",
        "display(Vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in vocab are 9092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'__PAD__': 0,\n",
              " '__</e>__': 1,\n",
              " '__UNK__': 2,\n",
              " 'followfriday': 3,\n",
              " 'top': 4,\n",
              " 'engag': 5,\n",
              " 'member': 6,\n",
              " 'commun': 7,\n",
              " 'week': 8,\n",
              " ':)': 9,\n",
              " 'hey': 10,\n",
              " 'jame': 11,\n",
              " 'odd': 12,\n",
              " ':/': 13,\n",
              " 'pleas': 14,\n",
              " 'call': 15,\n",
              " 'contact': 16,\n",
              " 'centr': 17,\n",
              " '02392441234': 18,\n",
              " 'abl': 19,\n",
              " 'assist': 20,\n",
              " 'mani': 21,\n",
              " 'thank': 22,\n",
              " 'listen': 23,\n",
              " 'last': 24,\n",
              " 'night': 25,\n",
              " 'bleed': 26,\n",
              " 'amaz': 27,\n",
              " 'track': 28,\n",
              " 'scotland': 29,\n",
              " 'congrat': 30,\n",
              " 'yeaaah': 31,\n",
              " 'yipppi': 32,\n",
              " 'accnt': 33,\n",
              " 'verifi': 34,\n",
              " 'rqst': 35,\n",
              " 'succeed': 36,\n",
              " 'got': 37,\n",
              " 'blue': 38,\n",
              " 'tick': 39,\n",
              " 'mark': 40,\n",
              " 'fb': 41,\n",
              " 'profil': 42,\n",
              " '15': 43,\n",
              " 'day': 44,\n",
              " 'one': 45,\n",
              " 'irresist': 46,\n",
              " 'flipkartfashionfriday': 47,\n",
              " 'like': 48,\n",
              " 'keep': 49,\n",
              " 'love': 50,\n",
              " 'custom': 51,\n",
              " 'wait': 52,\n",
              " 'long': 53,\n",
              " 'hope': 54,\n",
              " 'enjoy': 55,\n",
              " 'happi': 56,\n",
              " 'friday': 57,\n",
              " 'lwwf': 58,\n",
              " 'second': 59,\n",
              " 'thought': 60,\n",
              " '‚Äô': 61,\n",
              " 'enough': 62,\n",
              " 'time': 63,\n",
              " 'dd': 64,\n",
              " 'new': 65,\n",
              " 'short': 66,\n",
              " 'enter': 67,\n",
              " 'system': 68,\n",
              " 'sheep': 69,\n",
              " 'must': 70,\n",
              " 'buy': 71,\n",
              " 'jgh': 72,\n",
              " 'go': 73,\n",
              " 'bayan': 74,\n",
              " ':D': 75,\n",
              " 'bye': 76,\n",
              " 'act': 77,\n",
              " 'mischiev': 78,\n",
              " 'etl': 79,\n",
              " 'layer': 80,\n",
              " 'in-hous': 81,\n",
              " 'wareh': 82,\n",
              " 'app': 83,\n",
              " 'katamari': 84,\n",
              " 'well': 85,\n",
              " '‚Ä¶': 86,\n",
              " 'name': 87,\n",
              " 'impli': 88,\n",
              " ':p': 89,\n",
              " 'influenc': 90,\n",
              " 'big': 91,\n",
              " '...': 92,\n",
              " 'juici': 93,\n",
              " 'selfi': 94,\n",
              " 'follow': 95,\n",
              " 'perfect': 96,\n",
              " 'alreadi': 97,\n",
              " 'know': 98,\n",
              " \"what'\": 99,\n",
              " 'great': 100,\n",
              " 'opportun': 101,\n",
              " 'junior': 102,\n",
              " 'triathlet': 103,\n",
              " 'age': 104,\n",
              " '12': 105,\n",
              " '13': 106,\n",
              " 'gatorad': 107,\n",
              " 'seri': 108,\n",
              " 'get': 109,\n",
              " 'entri': 110,\n",
              " 'lay': 111,\n",
              " 'greet': 112,\n",
              " 'card': 113,\n",
              " 'rang': 114,\n",
              " 'print': 115,\n",
              " 'today': 116,\n",
              " 'job': 117,\n",
              " ':-)': 118,\n",
              " \"friend'\": 119,\n",
              " 'lunch': 120,\n",
              " 'yummm': 121,\n",
              " 'nostalgia': 122,\n",
              " 'tb': 123,\n",
              " 'ku': 124,\n",
              " 'id': 125,\n",
              " 'conflict': 126,\n",
              " 'help': 127,\n",
              " \"here'\": 128,\n",
              " 'screenshot': 129,\n",
              " 'work': 130,\n",
              " 'hi': 131,\n",
              " 'liv': 132,\n",
              " 'hello': 133,\n",
              " 'need': 134,\n",
              " 'someth': 135,\n",
              " 'u': 136,\n",
              " 'fm': 137,\n",
              " 'twitter': 138,\n",
              " '‚Äî': 139,\n",
              " 'sure': 140,\n",
              " 'thing': 141,\n",
              " 'dm': 142,\n",
              " 'x': 143,\n",
              " \"i'v\": 144,\n",
              " 'heard': 145,\n",
              " 'four': 146,\n",
              " 'season': 147,\n",
              " 'pretti': 148,\n",
              " 'dope': 149,\n",
              " 'penthous': 150,\n",
              " 'obv': 151,\n",
              " 'gobigorgohom': 152,\n",
              " 'fun': 153,\n",
              " \"y'all\": 154,\n",
              " 'yeah': 155,\n",
              " 'suppos': 156,\n",
              " 'lol': 157,\n",
              " 'chat': 158,\n",
              " 'bit': 159,\n",
              " 'youth': 160,\n",
              " 'üíÖ': 161,\n",
              " 'üèΩ': 162,\n",
              " 'üíã': 163,\n",
              " 'seen': 164,\n",
              " 'year': 165,\n",
              " 'rest': 166,\n",
              " 'goe': 167,\n",
              " 'quickli': 168,\n",
              " 'bed': 169,\n",
              " 'music': 170,\n",
              " 'fix': 171,\n",
              " 'dream': 172,\n",
              " 'spiritu': 173,\n",
              " 'ritual': 174,\n",
              " 'festiv': 175,\n",
              " 'n√©pal': 176,\n",
              " 'begin': 177,\n",
              " 'line-up': 178,\n",
              " 'left': 179,\n",
              " 'see': 180,\n",
              " 'sarah': 181,\n",
              " 'send': 182,\n",
              " 'us': 183,\n",
              " 'email': 184,\n",
              " 'bitsy@bitdefender.com': 185,\n",
              " \"we'll\": 186,\n",
              " 'asap': 187,\n",
              " 'kik': 188,\n",
              " 'hatessuc': 189,\n",
              " '32429': 190,\n",
              " 'kikm': 191,\n",
              " 'lgbt': 192,\n",
              " 'tinder': 193,\n",
              " 'nsfw': 194,\n",
              " 'akua': 195,\n",
              " 'cumshot': 196,\n",
              " 'come': 197,\n",
              " 'hous': 198,\n",
              " 'nsn_supplement': 199,\n",
              " 'effect': 200,\n",
              " 'press': 201,\n",
              " 'releas': 202,\n",
              " 'distribut': 203,\n",
              " 'result': 204,\n",
              " 'link': 205,\n",
              " 'remov': 206,\n",
              " 'pressreleas': 207,\n",
              " 'newsdistribut': 208,\n",
              " 'bam': 209,\n",
              " 'bestfriend': 210,\n",
              " 'lot': 211,\n",
              " 'warsaw': 212,\n",
              " '<3': 213,\n",
              " 'x46': 214,\n",
              " 'everyon': 215,\n",
              " 'watch': 216,\n",
              " 'documentari': 217,\n",
              " 'earthl': 218,\n",
              " 'youtub': 219,\n",
              " 'support': 220,\n",
              " 'buuut': 221,\n",
              " 'oh': 222,\n",
              " 'look': 223,\n",
              " 'forward': 224,\n",
              " 'visit': 225,\n",
              " 'next': 226,\n",
              " 'letsgetmessi': 227,\n",
              " 'jo': 228,\n",
              " 'make': 229,\n",
              " 'feel': 230,\n",
              " 'better': 231,\n",
              " 'never': 232,\n",
              " 'anyon': 233,\n",
              " 'kpop': 234,\n",
              " 'flesh': 235,\n",
              " 'good': 236,\n",
              " 'girl': 237,\n",
              " 'best': 238,\n",
              " 'wish': 239,\n",
              " 'reason': 240,\n",
              " 'epic': 241,\n",
              " 'soundtrack': 242,\n",
              " 'shout': 243,\n",
              " 'ad': 244,\n",
              " 'video': 245,\n",
              " 'playlist': 246,\n",
              " 'would': 247,\n",
              " 'dear': 248,\n",
              " 'jordan': 249,\n",
              " 'okay': 250,\n",
              " 'fake': 251,\n",
              " 'gameplay': 252,\n",
              " ';)': 253,\n",
              " 'haha': 254,\n",
              " 'im': 255,\n",
              " 'kid': 256,\n",
              " 'stuff': 257,\n",
              " 'exactli': 258,\n",
              " 'product': 259,\n",
              " 'line': 260,\n",
              " 'etsi': 261,\n",
              " 'shop': 262,\n",
              " 'check': 263,\n",
              " 'vacat': 264,\n",
              " 'recharg': 265,\n",
              " 'normal': 266,\n",
              " 'charger': 267,\n",
              " 'asleep': 268,\n",
              " 'talk': 269,\n",
              " 'sooo': 270,\n",
              " 'someon': 271,\n",
              " 'text': 272,\n",
              " 'ye': 273,\n",
              " 'bet': 274,\n",
              " \"he'll\": 275,\n",
              " 'fit': 276,\n",
              " 'hear': 277,\n",
              " 'speech': 278,\n",
              " 'piti': 279,\n",
              " 'green': 280,\n",
              " 'garden': 281,\n",
              " 'midnight': 282,\n",
              " 'sun': 283,\n",
              " 'beauti': 284,\n",
              " 'canal': 285,\n",
              " 'dasvidaniya': 286,\n",
              " 'till': 287,\n",
              " 'scout': 288,\n",
              " 'sg': 289,\n",
              " 'futur': 290,\n",
              " 'wlan': 291,\n",
              " 'pro': 292,\n",
              " 'confer': 293,\n",
              " 'asia': 294,\n",
              " 'chang': 295,\n",
              " 'lollipop': 296,\n",
              " 'üç≠': 297,\n",
              " 'nez': 298,\n",
              " 'agnezmo': 299,\n",
              " 'oley': 300,\n",
              " 'mama': 301,\n",
              " 'stand': 302,\n",
              " 'stronger': 303,\n",
              " 'god': 304,\n",
              " 'misti': 305,\n",
              " 'babi': 306,\n",
              " 'cute': 307,\n",
              " 'woohoo': 308,\n",
              " \"can't\": 309,\n",
              " 'sign': 310,\n",
              " 'yet': 311,\n",
              " 'still': 312,\n",
              " 'think': 313,\n",
              " 'mka': 314,\n",
              " 'liam': 315,\n",
              " 'access': 316,\n",
              " 'welcom': 317,\n",
              " 'stat': 318,\n",
              " 'arriv': 319,\n",
              " '1': 320,\n",
              " 'unfollow': 321,\n",
              " 'via': 322,\n",
              " 'surpris': 323,\n",
              " 'figur': 324,\n",
              " 'happybirthdayemilybett': 325,\n",
              " 'sweet': 326,\n",
              " 'talent': 327,\n",
              " '2': 328,\n",
              " 'plan': 329,\n",
              " 'drain': 330,\n",
              " 'gotta': 331,\n",
              " 'timezon': 332,\n",
              " 'parent': 333,\n",
              " 'proud': 334,\n",
              " 'least': 335,\n",
              " 'mayb': 336,\n",
              " 'sometim': 337,\n",
              " 'grade': 338,\n",
              " 'al': 339,\n",
              " 'grand': 340,\n",
              " 'manila_bro': 341,\n",
              " 'chosen': 342,\n",
              " 'let': 343,\n",
              " 'around': 344,\n",
              " '..': 345,\n",
              " 'side': 346,\n",
              " 'world': 347,\n",
              " 'eh': 348,\n",
              " 'take': 349,\n",
              " 'care': 350,\n",
              " 'final': 351,\n",
              " 'fuck': 352,\n",
              " 'weekend': 353,\n",
              " 'real': 354,\n",
              " 'x45': 355,\n",
              " 'join': 356,\n",
              " 'hushedcallwithfraydo': 357,\n",
              " 'gift': 358,\n",
              " 'yeahhh': 359,\n",
              " 'hushedpinwithsammi': 360,\n",
              " 'event': 361,\n",
              " 'might': 362,\n",
              " 'luv': 363,\n",
              " 'realli': 364,\n",
              " 'appreci': 365,\n",
              " 'share': 366,\n",
              " 'wow': 367,\n",
              " 'tom': 368,\n",
              " 'gym': 369,\n",
              " 'monday': 370,\n",
              " 'invit': 371,\n",
              " 'scope': 372,\n",
              " 'friend': 373,\n",
              " 'nude': 374,\n",
              " 'sleep': 375,\n",
              " 'birthday': 376,\n",
              " 'want': 377,\n",
              " 't-shirt': 378,\n",
              " 'cool': 379,\n",
              " 'haw': 380,\n",
              " 'phela': 381,\n",
              " 'mom': 382,\n",
              " 'obvious': 383,\n",
              " 'princ': 384,\n",
              " 'charm': 385,\n",
              " 'stage': 386,\n",
              " 'luck': 387,\n",
              " 'tyler': 388,\n",
              " 'hipster': 389,\n",
              " 'glass': 390,\n",
              " 'marti': 391,\n",
              " 'glad': 392,\n",
              " 'done': 393,\n",
              " 'afternoon': 394,\n",
              " 'read': 395,\n",
              " 'kahfi': 396,\n",
              " 'finish': 397,\n",
              " 'ohmyg': 398,\n",
              " 'yaya': 399,\n",
              " 'dub': 400,\n",
              " 'stalk': 401,\n",
              " 'ig': 402,\n",
              " 'gondooo': 403,\n",
              " 'moo': 404,\n",
              " 'tologooo': 405,\n",
              " 'becom': 406,\n",
              " 'detail': 407,\n",
              " 'zzz': 408,\n",
              " 'xx': 409,\n",
              " 'physiotherapi': 410,\n",
              " 'hashtag': 411,\n",
              " 'üí™': 412,\n",
              " 'monica': 413,\n",
              " 'miss': 414,\n",
              " 'sound': 415,\n",
              " 'morn': 416,\n",
              " \"that'\": 417,\n",
              " 'x43': 418,\n",
              " 'definit': 419,\n",
              " 'tri': 420,\n",
              " 'tonight': 421,\n",
              " 'took': 422,\n",
              " 'advic': 423,\n",
              " 'treviso': 424,\n",
              " 'concert': 425,\n",
              " 'citi': 426,\n",
              " 'countri': 427,\n",
              " \"i'll\": 428,\n",
              " 'start': 429,\n",
              " 'fine': 430,\n",
              " 'gorgeou': 431,\n",
              " 'xo': 432,\n",
              " 'oven': 433,\n",
              " 'roast': 434,\n",
              " 'garlic': 435,\n",
              " 'oliv': 436,\n",
              " 'oil': 437,\n",
              " 'dri': 438,\n",
              " 'tomato': 439,\n",
              " 'basil': 440,\n",
              " 'centuri': 441,\n",
              " 'tuna': 442,\n",
              " 'right': 443,\n",
              " 'back': 444,\n",
              " 'atchya': 445,\n",
              " 'even': 446,\n",
              " 'almost': 447,\n",
              " 'chanc': 448,\n",
              " 'cheer': 449,\n",
              " 'po': 450,\n",
              " 'ice': 451,\n",
              " 'cream': 452,\n",
              " 'agre': 453,\n",
              " '100': 454,\n",
              " 'heheheh': 455,\n",
              " 'that': 456,\n",
              " 'point': 457,\n",
              " 'stay': 458,\n",
              " 'home': 459,\n",
              " 'soon': 460,\n",
              " 'promis': 461,\n",
              " 'web': 462,\n",
              " 'whatsapp': 463,\n",
              " 'volta': 464,\n",
              " 'funcionar': 465,\n",
              " 'com': 466,\n",
              " 'iphon': 467,\n",
              " 'jailbroken': 468,\n",
              " 'later': 469,\n",
              " '34': 470,\n",
              " 'min': 471,\n",
              " 'leia': 472,\n",
              " 'appear': 473,\n",
              " 'hologram': 474,\n",
              " 'r2d2': 475,\n",
              " 'w': 476,\n",
              " 'messag': 477,\n",
              " 'obi': 478,\n",
              " 'wan': 479,\n",
              " 'sit': 480,\n",
              " 'luke': 481,\n",
              " 'inter': 482,\n",
              " '3': 483,\n",
              " 'ucl': 484,\n",
              " 'arsen': 485,\n",
              " 'small': 486,\n",
              " 'team': 487,\n",
              " 'pass': 488,\n",
              " 'üöÇ': 489,\n",
              " 'dewsburi': 490,\n",
              " 'railway': 491,\n",
              " 'station': 492,\n",
              " 'dew': 493,\n",
              " 'west': 494,\n",
              " 'yorkshir': 495,\n",
              " '430': 496,\n",
              " 'smh': 497,\n",
              " '9:25': 498,\n",
              " 'live': 499,\n",
              " 'strang': 500,\n",
              " 'imagin': 501,\n",
              " 'megan': 502,\n",
              " 'masaantoday': 503,\n",
              " 'a4': 504,\n",
              " 'shweta': 505,\n",
              " 'tripathi': 506,\n",
              " '5': 507,\n",
              " '20': 508,\n",
              " 'kurta': 509,\n",
              " 'half': 510,\n",
              " 'number': 511,\n",
              " 'wsalelov': 512,\n",
              " 'ah': 513,\n",
              " 'larri': 514,\n",
              " 'anyway': 515,\n",
              " 'kinda': 516,\n",
              " 'goood': 517,\n",
              " 'life': 518,\n",
              " 'enn': 519,\n",
              " 'could': 520,\n",
              " 'warmup': 521,\n",
              " '15th': 522,\n",
              " 'bath': 523,\n",
              " 'dum': 524,\n",
              " 'andar': 525,\n",
              " 'ram': 526,\n",
              " 'sampath': 527,\n",
              " 'sona': 528,\n",
              " 'mohapatra': 529,\n",
              " 'samantha': 530,\n",
              " 'edward': 531,\n",
              " 'mein': 532,\n",
              " 'tulan': 533,\n",
              " 'razi': 534,\n",
              " 'wah': 535,\n",
              " 'josh': 536,\n",
              " 'alway': 537,\n",
              " 'smile': 538,\n",
              " 'pictur': 539,\n",
              " '16.20': 540,\n",
              " 'giveitup': 541,\n",
              " 'given': 542,\n",
              " 'ga': 543,\n",
              " 'subsidi': 544,\n",
              " 'initi': 545,\n",
              " 'propos': 546,\n",
              " 'delight': 547,\n",
              " 'yesterday': 548,\n",
              " 'x42': 549,\n",
              " 'lmaoo': 550,\n",
              " 'song': 551,\n",
              " 'ever': 552,\n",
              " 'shall': 553,\n",
              " 'littl': 554,\n",
              " 'throwback': 555,\n",
              " 'outli': 556,\n",
              " 'island': 557,\n",
              " 'cheung': 558,\n",
              " 'chau': 559,\n",
              " 'mui': 560,\n",
              " 'wo': 561,\n",
              " 'total': 562,\n",
              " 'differ': 563,\n",
              " 'kfckitchentour': 564,\n",
              " 'kitchen': 565,\n",
              " 'clean': 566,\n",
              " \"i'm\": 567,\n",
              " 'cusp': 568,\n",
              " 'test': 569,\n",
              " 'water': 570,\n",
              " 'reward': 571,\n",
              " 'arummzz': 572,\n",
              " \"let'\": 573,\n",
              " 'drive': 574,\n",
              " 'travel': 575,\n",
              " 'yogyakarta': 576,\n",
              " 'jeep': 577,\n",
              " 'indonesia': 578,\n",
              " 'instamood': 579,\n",
              " 'wanna': 580,\n",
              " 'skype': 581,\n",
              " 'may': 582,\n",
              " 'nice': 583,\n",
              " 'friendli': 584,\n",
              " 'pretend': 585,\n",
              " 'film': 586,\n",
              " 'congratul': 587,\n",
              " 'winner': 588,\n",
              " 'cheesydelight': 589,\n",
              " 'contest': 590,\n",
              " 'address': 591,\n",
              " 'guy': 592,\n",
              " 'market': 593,\n",
              " '24/7': 594,\n",
              " '14': 595,\n",
              " 'hour': 596,\n",
              " 'leav': 597,\n",
              " 'without': 598,\n",
              " 'delay': 599,\n",
              " 'actual': 600,\n",
              " 'easi': 601,\n",
              " 'guess': 602,\n",
              " 'train': 603,\n",
              " 'wd': 604,\n",
              " 'shift': 605,\n",
              " 'engin': 606,\n",
              " 'etc': 607,\n",
              " 'sunburn': 608,\n",
              " 'peel': 609,\n",
              " 'blog': 610,\n",
              " 'huge': 611,\n",
              " 'warm': 612,\n",
              " '‚òÜ': 613,\n",
              " 'complet': 614,\n",
              " 'triangl': 615,\n",
              " 'northern': 616,\n",
              " 'ireland': 617,\n",
              " 'sight': 618,\n",
              " 'smthng': 619,\n",
              " 'fr': 620,\n",
              " 'hug': 621,\n",
              " 'xoxo': 622,\n",
              " 'uu': 623,\n",
              " 'jaann': 624,\n",
              " 'topnewfollow': 625,\n",
              " 'connect': 626,\n",
              " 'wonder': 627,\n",
              " 'made': 628,\n",
              " 'fluffi': 629,\n",
              " 'insid': 630,\n",
              " 'pirouett': 631,\n",
              " 'moos': 632,\n",
              " 'trip': 633,\n",
              " 'philli': 634,\n",
              " 'decemb': 635,\n",
              " \"i'd\": 636,\n",
              " 'dude': 637,\n",
              " 'x41': 638,\n",
              " 'question': 639,\n",
              " 'flaw': 640,\n",
              " 'pain': 641,\n",
              " 'negat': 642,\n",
              " 'strength': 643,\n",
              " 'went': 644,\n",
              " 'solo': 645,\n",
              " 'move': 646,\n",
              " 'fav': 647,\n",
              " 'nirvana': 648,\n",
              " 'smell': 649,\n",
              " 'teen': 650,\n",
              " 'spirit': 651,\n",
              " 'rip': 652,\n",
              " 'ami': 653,\n",
              " 'winehous': 654,\n",
              " 'coupl': 655,\n",
              " 'tomhiddleston': 656,\n",
              " 'elizabetholsen': 657,\n",
              " 'yaytheylookgreat': 658,\n",
              " 'goodnight': 659,\n",
              " 'vid': 660,\n",
              " 'wake': 661,\n",
              " 'gonna': 662,\n",
              " 'shoot': 663,\n",
              " 'itti': 664,\n",
              " 'bitti': 665,\n",
              " 'teeni': 666,\n",
              " 'bikini': 667,\n",
              " 'much': 668,\n",
              " '4th': 669,\n",
              " 'togeth': 670,\n",
              " 'end': 671,\n",
              " 'xfile': 672,\n",
              " 'content': 673,\n",
              " 'rain': 674,\n",
              " 'fabul': 675,\n",
              " 'fantast': 676,\n",
              " '‚ô°': 677,\n",
              " 'jb': 678,\n",
              " 'forev': 679,\n",
              " 'belieb': 680,\n",
              " 'nighti': 681,\n",
              " 'bug': 682,\n",
              " 'bite': 683,\n",
              " 'bracelet': 684,\n",
              " 'idea': 685,\n",
              " 'foundri': 686,\n",
              " 'game': 687,\n",
              " 'sens': 688,\n",
              " 'pic': 689,\n",
              " 'ef': 690,\n",
              " 'phone': 691,\n",
              " 'woot': 692,\n",
              " 'derek': 693,\n",
              " 'use': 694,\n",
              " 'parkshar': 695,\n",
              " 'gloucestershir': 696,\n",
              " 'aaaahhh': 697,\n",
              " 'man': 698,\n",
              " 'traffic': 699,\n",
              " 'stress': 700,\n",
              " 'reliev': 701,\n",
              " \"how'r\": 702,\n",
              " 'arbeloa': 703,\n",
              " 'turn': 704,\n",
              " '17': 705,\n",
              " 'omg': 706,\n",
              " 'say': 707,\n",
              " 'europ': 708,\n",
              " 'rise': 709,\n",
              " 'find': 710,\n",
              " 'hard': 711,\n",
              " 'believ': 712,\n",
              " 'uncount': 713,\n",
              " 'coz': 714,\n",
              " 'unlimit': 715,\n",
              " 'cours': 716,\n",
              " 'teamposit': 717,\n",
              " 'aldub': 718,\n",
              " '‚òï': 719,\n",
              " 'rita': 720,\n",
              " 'info': 721,\n",
              " \"we'd\": 722,\n",
              " 'way': 723,\n",
              " 'boy': 724,\n",
              " 'x40': 725,\n",
              " 'true': 726,\n",
              " 'sethi': 727,\n",
              " 'high': 728,\n",
              " 'exe': 729,\n",
              " 'skeem': 730,\n",
              " 'saam': 731,\n",
              " 'peopl': 732,\n",
              " 'polit': 733,\n",
              " 'izzat': 734,\n",
              " 'wese': 735,\n",
              " 'trust': 736,\n",
              " 'khawateen': 737,\n",
              " 'k': 738,\n",
              " 'sath': 739,\n",
              " 'mana': 740,\n",
              " 'kar': 741,\n",
              " 'deya': 742,\n",
              " 'sort': 743,\n",
              " 'smart': 744,\n",
              " 'hair': 745,\n",
              " 'tbh': 746,\n",
              " 'jacob': 747,\n",
              " 'g': 748,\n",
              " 'upgrad': 749,\n",
              " 'tee': 750,\n",
              " 'famili': 751,\n",
              " 'person': 752,\n",
              " 'two': 753,\n",
              " 'convers': 754,\n",
              " 'onlin': 755,\n",
              " 'mclaren': 756,\n",
              " 'fridayfeel': 757,\n",
              " 'tgif': 758,\n",
              " 'squar': 759,\n",
              " 'enix': 760,\n",
              " 'bissmillah': 761,\n",
              " 'ya': 762,\n",
              " 'allah': 763,\n",
              " \"we'r\": 764,\n",
              " 'socent': 765,\n",
              " 'startup': 766,\n",
              " 'drop': 767,\n",
              " 'your': 768,\n",
              " 'arnd': 769,\n",
              " 'town': 770,\n",
              " 'basic': 771,\n",
              " 'piss': 772,\n",
              " 'cup': 773,\n",
              " 'also': 774,\n",
              " 'terribl': 775,\n",
              " 'complic': 776,\n",
              " 'discuss': 777,\n",
              " 'snapchat': 778,\n",
              " 'lynettelow': 779,\n",
              " 'kikmenow': 780,\n",
              " 'snapm': 781,\n",
              " 'hot': 782,\n",
              " 'amazon': 783,\n",
              " 'kikmeguy': 784,\n",
              " 'defin': 785,\n",
              " 'grow': 786,\n",
              " 'sport': 787,\n",
              " 'rt': 788,\n",
              " 'rakyat': 789,\n",
              " 'write': 790,\n",
              " 'sinc': 791,\n",
              " 'mention': 792,\n",
              " 'fli': 793,\n",
              " 'fish': 794,\n",
              " 'promot': 795,\n",
              " 'post': 796,\n",
              " 'cyber': 797,\n",
              " 'ourdaughtersourprid': 798,\n",
              " 'mypapamyprid': 799,\n",
              " 'papa': 800,\n",
              " 'coach': 801,\n",
              " 'posit': 802,\n",
              " 'kha': 803,\n",
              " 'atleast': 804,\n",
              " 'x39': 805,\n",
              " 'mango': 806,\n",
              " \"lassi'\": 807,\n",
              " \"monty'\": 808,\n",
              " 'marvel': 809,\n",
              " 'though': 810,\n",
              " 'suspect': 811,\n",
              " 'meant': 812,\n",
              " '24': 813,\n",
              " 'hr': 814,\n",
              " 'touch': 815,\n",
              " 'kepler': 816,\n",
              " '452b': 817,\n",
              " 'chalna': 818,\n",
              " 'hai': 819,\n",
              " 'thankyou': 820,\n",
              " 'hazel': 821,\n",
              " 'food': 822,\n",
              " 'brooklyn': 823,\n",
              " 'pta': 824,\n",
              " 'awak': 825,\n",
              " 'okayi': 826,\n",
              " 'awww': 827,\n",
              " 'ha': 828,\n",
              " 'doc': 829,\n",
              " 'splendid': 830,\n",
              " 'spam': 831,\n",
              " 'folder': 832,\n",
              " 'amount': 833,\n",
              " 'nigeria': 834,\n",
              " 'claim': 835,\n",
              " 'rted': 836,\n",
              " 'leg': 837,\n",
              " 'hurt': 838,\n",
              " 'bad': 839,\n",
              " 'mine': 840,\n",
              " 'saturday': 841,\n",
              " 'thaaank': 842,\n",
              " 'puhon': 843,\n",
              " 'happinesss': 844,\n",
              " 'tnc': 845,\n",
              " 'prior': 846,\n",
              " 'notif': 847,\n",
              " 'fat': 848,\n",
              " 'co': 849,\n",
              " 'probabl': 850,\n",
              " 'ate': 851,\n",
              " 'yuna': 852,\n",
              " 'tamesid': 853,\n",
              " '¬¥': 854,\n",
              " 'googl': 855,\n",
              " 'account': 856,\n",
              " 'scouser': 857,\n",
              " 'everyth': 858,\n",
              " 'zoe': 859,\n",
              " 'mate': 860,\n",
              " 'liter': 861,\n",
              " \"they'r\": 862,\n",
              " 'samee': 863,\n",
              " 'edgar': 864,\n",
              " 'updat': 865,\n",
              " 'log': 866,\n",
              " 'bring': 867,\n",
              " 'abe': 868,\n",
              " 'meet': 869,\n",
              " 'x38': 870,\n",
              " 'sigh': 871,\n",
              " 'dreamili': 872,\n",
              " 'pout': 873,\n",
              " 'eye': 874,\n",
              " 'quacketyquack': 875,\n",
              " 'funni': 876,\n",
              " 'happen': 877,\n",
              " 'phil': 878,\n",
              " 'em': 879,\n",
              " 'del': 880,\n",
              " 'rodder': 881,\n",
              " 'els': 882,\n",
              " 'play': 883,\n",
              " 'newest': 884,\n",
              " 'gamejam': 885,\n",
              " 'irish': 886,\n",
              " 'literatur': 887,\n",
              " 'inaccess': 888,\n",
              " \"kareena'\": 889,\n",
              " 'fan': 890,\n",
              " 'brain': 891,\n",
              " 'dot': 892,\n",
              " 'braindot': 893,\n",
              " 'fair': 894,\n",
              " 'rush': 895,\n",
              " 'either': 896,\n",
              " 'brandi': 897,\n",
              " '18': 898,\n",
              " 'carniv': 899,\n",
              " 'men': 900,\n",
              " 'put': 901,\n",
              " 'mask': 902,\n",
              " 'xavier': 903,\n",
              " 'forneret': 904,\n",
              " 'jennif': 905,\n",
              " 'site': 906,\n",
              " 'free': 907,\n",
              " '50.000': 908,\n",
              " '8': 909,\n",
              " 'ball': 910,\n",
              " 'pool': 911,\n",
              " 'coin': 912,\n",
              " 'edit': 913,\n",
              " 'trish': 914,\n",
              " '‚ô•': 915,\n",
              " 'grate': 916,\n",
              " 'three': 917,\n",
              " 'comment': 918,\n",
              " 'wakeup': 919,\n",
              " 'besid': 920,\n",
              " 'dirti': 921,\n",
              " 'sex': 922,\n",
              " 'lmaooo': 923,\n",
              " 'üò§': 924,\n",
              " 'loui': 925,\n",
              " \"he'\": 926,\n",
              " 'throw': 927,\n",
              " 'caus': 928,\n",
              " 'inspir': 929,\n",
              " 'ff': 930,\n",
              " 'twoof': 931,\n",
              " 'gr8': 932,\n",
              " 'wkend': 933,\n",
              " 'kind': 934,\n",
              " 'exhaust': 935,\n",
              " 'word': 936,\n",
              " 'cheltenham': 937,\n",
              " 'area': 938,\n",
              " 'kale': 939,\n",
              " 'crisp': 940,\n",
              " 'ruin': 941,\n",
              " 'x37': 942,\n",
              " 'open': 943,\n",
              " 'worldwid': 944,\n",
              " 'outta': 945,\n",
              " 'sfvbeta': 946,\n",
              " 'vantast': 947,\n",
              " 'xcylin': 948,\n",
              " 'bundl': 949,\n",
              " 'show': 950,\n",
              " 'internet': 951,\n",
              " 'price': 952,\n",
              " 'realisticli': 953,\n",
              " 'pay': 954,\n",
              " 'net': 955,\n",
              " 'educ': 956,\n",
              " 'power': 957,\n",
              " 'weapon': 958,\n",
              " 'nelson': 959,\n",
              " 'mandela': 960,\n",
              " 'recent': 961,\n",
              " 'j': 962,\n",
              " 'chenab': 963,\n",
              " 'flow': 964,\n",
              " 'pakistan': 965,\n",
              " 'incredibleindia': 966,\n",
              " 'teenchoic': 967,\n",
              " 'choiceinternationalartist': 968,\n",
              " 'superjunior': 969,\n",
              " 'caught': 970,\n",
              " 'first': 971,\n",
              " 'salmon': 972,\n",
              " 'super-blend': 973,\n",
              " 'project': 974,\n",
              " 'youth@bipolaruk.org.uk': 975,\n",
              " 'awesom': 976,\n",
              " 'stream': 977,\n",
              " 'alma': 978,\n",
              " 'mater': 979,\n",
              " 'highschoolday': 980,\n",
              " 'clientvisit': 981,\n",
              " 'faith': 982,\n",
              " 'christian': 983,\n",
              " 'school': 984,\n",
              " 'lizaminnelli': 985,\n",
              " 'upcom': 986,\n",
              " 'uk': 987,\n",
              " 'üòÑ': 988,\n",
              " 'singl': 989,\n",
              " 'hill': 990,\n",
              " 'everi': 991,\n",
              " 'beat': 992,\n",
              " 'wrong': 993,\n",
              " 'readi': 994,\n",
              " 'natur': 995,\n",
              " 'pefumeri': 996,\n",
              " 'workshop': 997,\n",
              " 'neal': 998,\n",
              " 'yard': 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXrrRrnvuITT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
        "  word_l = process_tweet(tweet)\n",
        "\n",
        "  if verbose:\n",
        "    print(\"List of words from the processed tweet:\")\n",
        "    print(world_l)\n",
        "\n",
        "  tensor_l = []\n",
        "\n",
        "  unk_ID = vocab_dict[unk_token]\n",
        "\n",
        "  if verbose:\n",
        "    print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
        "\n",
        "  for word in word_l:\n",
        "    word_ID = vocab_dict.get(word, unk_ID)\n",
        "\n",
        "    tensor_l.append(word_ID)\n",
        "\n",
        "  return tensor_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ebfgIu9x0BZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "cadb96d2-cd6e-440b-fcee-9bddaec3fdb0"
      },
      "source": [
        "print(\"Actual tweet is\\n\", val_pos[0])\n",
        "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual tweet is\n",
            " Bro:U wan cut hair anot,ur hair long Liao bo\n",
            "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
            "Bro:LOL Sibei xialan\n",
            "\n",
            "Tensor of tweet:\n",
            " [1065, 136, 479, 2351, 745, 8146, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxV-hwCWx_fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "  \n",
        "  # make sure the batch size is an even number\n",
        "  assert batch_size % 2 == 0\n",
        "\n",
        "  # Number of positive examples in each batch is half of the batch size\n",
        "  # same with number of negative examples in each batch\n",
        "  n_to_take = batch_size // 2\n",
        "\n",
        "  pos_index = 0\n",
        "  neg_index = 0\n",
        "\n",
        "  len_data_pos = len(data_pos)\n",
        "  len_data_neg = len(data_neg)\n",
        "\n",
        "  pos_index_lines = list(range(len_data_pos))\n",
        "  neg_index_lines = list(range(len_data_neg))\n",
        "\n",
        "  if shuffle:\n",
        "    rnd.shuffle(pos_index_lines)\n",
        "    rnd.shuffle(neg_index_lines)\n",
        "\n",
        "  stop = False\n",
        "\n",
        "  while not stop:\n",
        "\n",
        "    batch = []\n",
        "\n",
        "    for i in range(n_to_take):\n",
        "      if pos_index >= len_data_pos:\n",
        "        \n",
        "        if not loop:\n",
        "          stop = True\n",
        "          break\n",
        "\n",
        "        pos_index = 0\n",
        "\n",
        "        if shuffle:\n",
        "          rnd.shuffle(pos_index_lines)\n",
        "\n",
        "      tweet = data_pos[pos_index_lines[pos_index]]\n",
        "      tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "      batch.append(tensor)\n",
        "      pos_index = pos_index + 1\n",
        "\n",
        "\n",
        "    for i in range(n_to_take):\n",
        "      if neg_index >= len_data_neg:\n",
        "        if not loop:\n",
        "          stop = True\n",
        "          break\n",
        "\n",
        "        neg_index = 0\n",
        "        if shuffle:\n",
        "          rnd.shuffle(neg_index_lines)\n",
        "\n",
        "      tweet = data_neg[neg_index_lines[neg_index]]\n",
        "      tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "      batch.append(tensor)\n",
        "      neg_index = neg_index + 1\n",
        "\n",
        "    if stop:\n",
        "      break\n",
        "\n",
        "    pos_index += n_to_take\n",
        "    neg_index += n_to_take\n",
        "\n",
        "    max_len = max([len(t) for t in batch])\n",
        "\n",
        "    tensor_pad_l = []\n",
        "\n",
        "    for tensor in batch:\n",
        "      n_pad = max_len - len(tensor)\n",
        "      pad_l = [0] * n_pad\n",
        "      tensor_pad = tensor + pad_l\n",
        "      tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "    inputs = np.array(tensor_pad_l)\n",
        "    target_pos = [1] * (n_to_take)\n",
        "    target_neg = [0] * (n_to_take)\n",
        "    target_l = target_pos + target_neg\n",
        "    targets = np.array(target_l)\n",
        "    example_weights = np.ones_like(targets)\n",
        "\n",
        "    yield inputs, targets, example_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp-dg78L5dFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "32df139f-118d-42d6-bba0-53341936cfb6"
      },
      "source": [
        "rnd.seed(30) \n",
        "\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
        "\n",
        "\n",
        "def test_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0]\n",
            " [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n",
            " [3761  109  136  583 2930 3969    0    0    0    0    0]\n",
            " [ 250 3761    0    0    0    0    0    0    0    0    0]]\n",
            "Targets: [1 1 0 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv8hO_5M-EMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Layer(object):\n",
        "  def __init__(self):\n",
        "      self.weights = None\n",
        "\n",
        "  def forward(self, x):\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def init_weights_and_state(self, input_signature, random_key):\n",
        "      pass\n",
        "\n",
        "  def init(self, input_signature, random_key):\n",
        "      self.init_weights_and_state(input_signature, random_key)\n",
        "      return self.weights\n",
        "  \n",
        "  def __call__(self, x):\n",
        "      return self.forward(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRavq9hc-uFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReLu(Layer):\n",
        "  def forward(self, x):\n",
        "    activation = np.maximum(x, 0)\n",
        "\n",
        "    return activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C75BiwkM-8jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from trax import fastmath\n",
        "np = fastmath.numpy\n",
        "random = fastmath.random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0d4KpOu_QV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dense(Layer):\n",
        "  def __init__(self, n_units, init_stdev=0.1):\n",
        "    self._n_units = n_units\n",
        "    self._init_stdev = init_stdev\n",
        "  def forward(self, x):\n",
        "    dense = np.dot(x, self.weights)\n",
        "    return dense\n",
        "\n",
        "  def init_weights_and_state(self, input_signature, random_key):\n",
        "    input_shape = input_signature.shape\n",
        "    w = self._init_stdev * trax.fastmath.random.normal(key=random_key,  \n",
        "                                                      shape = (input_shape[-1], self._n_units))\n",
        "    self.weights = w\n",
        "    return self.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlfHsrjRlhC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "c937c424-2240-4e94-fbc5-87261127f0d4"
      },
      "source": [
        "# Testing your Dense layer \n",
        "dense_layer = Dense(n_units=10)  #sets  number of units in dense layer\n",
        "random_key = random.get_prng(seed=0)  # sets random seed\n",
        "z = np.array([[2.0, 7.0, 25.0]]) # input array \n",
        "\n",
        "dense_layer.init(z, random_key)\n",
        "print(\"Weights are\\n \",dense_layer.weights) #Returns randomly generated weights\n",
        "print(\"Foward function output is \", dense_layer(z)) # Returns multiplied values of units and weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights are\n",
            "  [[-0.02837107  0.09368163 -0.10050073  0.14165013  0.10543301  0.09108127\n",
            "  -0.04265671  0.0986188  -0.05575324  0.0015325 ]\n",
            " [-0.2078568   0.05548371  0.09142365  0.05744596  0.07227863  0.01210618\n",
            "  -0.03237354  0.16234998  0.02450039 -0.13809781]\n",
            " [-0.06111237  0.01403725  0.08410043 -0.10943579 -0.1077502  -0.11396457\n",
            "  -0.0593338  -0.01557651 -0.03832145 -0.11144515]]\n",
            "Foward function output is  [[-3.0395489   0.92668045  2.5414748  -2.0504727  -1.9769385  -2.5822086\n",
            "  -1.7952732   0.94427466 -0.89803994 -3.7497485 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whiOYmMNm88p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
        "  embed_layer = tl.Embedding(\n",
        "      vocab_size=vocab_size,\n",
        "      d_feature=embedding_dim)\n",
        "  \n",
        "  mean_layer = tl.Mean(axis=1)\n",
        "  dense_output_layer = tl.Dense(n_units = output_dim)\n",
        "  log_softmax_layer = tl.LogSoftmax()\n",
        "\n",
        "  model = tl.Serial(\n",
        "      embed_layer,\n",
        "      mean_layer,\n",
        "      dense_output_layer,\n",
        "      log_softmax_layer)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sbe-0OEoXD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "batch_size = 16\n",
        "rnd.seed(271)\n",
        "\n",
        "train_task =  training.TrainTask(\n",
        "    labeled_data = train_generator(batch_size=batch_size, shuffle=True),\n",
        "    loss_layer = tl.CrossEntropyLoss(),\n",
        "    optimizer = trax.optimizers.Adam(0.01),\n",
        "    n_steps_per_checkpoint = 10)\n",
        "\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()])\n",
        "\n",
        "model = classifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbyaapXRDXI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dir_expand = os.path.expanduser('~/output_dir/')\n",
        "!rm -rf {output_dir_expand}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5RNHRLoqtBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "  training_loop = training.Loop(\n",
        "                                classifier, # The learning model\n",
        "                                train_task, # The training task\n",
        "                                eval_tasks = [eval_task],# The evaluation task\n",
        "                                output_dir = output_dir) # The output directory\n",
        "  \n",
        "  training_loop.run(n_steps = n_steps)\n",
        "\n",
        "  return training_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2YeHRjir8Ek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d88662b9-a503-452c-819a-20918abf7545"
      },
      "source": [
        "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Ran 1 train steps in 2.25 secs\n",
            "Step      1: train CrossEntropyLoss |  1.26101446\n",
            "Step      1: eval  CrossEntropyLoss |  0.77992326\n",
            "Step      1: eval          Accuracy |  0.43750000\n",
            "\n",
            "Step     10: Ran 9 train steps in 3.78 secs\n",
            "Step     10: train CrossEntropyLoss |  0.78989428\n",
            "Step     10: eval  CrossEntropyLoss |  0.70014697\n",
            "Step     10: eval          Accuracy |  0.50000000\n",
            "\n",
            "Step     20: Ran 10 train steps in 2.23 secs\n",
            "Step     20: train CrossEntropyLoss |  0.46327835\n",
            "Step     20: eval  CrossEntropyLoss |  0.36245325\n",
            "Step     20: eval          Accuracy |  0.87500000\n",
            "\n",
            "Step     30: Ran 10 train steps in 1.76 secs\n",
            "Step     30: train CrossEntropyLoss |  0.30180237\n",
            "Step     30: eval  CrossEntropyLoss |  0.23907390\n",
            "Step     30: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     40: Ran 10 train steps in 1.18 secs\n",
            "Step     40: train CrossEntropyLoss |  0.26077047\n",
            "Step     40: eval  CrossEntropyLoss |  0.35614920\n",
            "Step     40: eval          Accuracy |  0.68750000\n",
            "\n",
            "Step     50: Ran 10 train steps in 0.71 secs\n",
            "Step     50: train CrossEntropyLoss |  0.16338134\n",
            "Step     50: eval  CrossEntropyLoss |  0.19215865\n",
            "Step     50: eval          Accuracy |  0.93750000\n",
            "\n",
            "Step     60: Ran 10 train steps in 1.22 secs\n",
            "Step     60: train CrossEntropyLoss |  0.12189268\n",
            "Step     60: eval  CrossEntropyLoss |  0.12328621\n",
            "Step     60: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     70: Ran 10 train steps in 1.07 secs\n",
            "Step     70: train CrossEntropyLoss |  0.09908404\n",
            "Step     70: eval  CrossEntropyLoss |  0.10022088\n",
            "Step     70: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     80: Ran 10 train steps in 0.74 secs\n",
            "Step     80: train CrossEntropyLoss |  0.09108972\n",
            "Step     80: eval  CrossEntropyLoss |  0.04984755\n",
            "Step     80: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     90: Ran 10 train steps in 1.37 secs\n",
            "Step     90: train CrossEntropyLoss |  0.06861026\n",
            "Step     90: eval  CrossEntropyLoss |  0.01920820\n",
            "Step     90: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step    100: Ran 10 train steps in 0.78 secs\n",
            "Step    100: train CrossEntropyLoss |  0.05282158\n",
            "Step    100: eval  CrossEntropyLoss |  0.00219619\n",
            "Step    100: eval          Accuracy |  1.00000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKJpsB7EHkmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_accuracy(preds, y, y_weights):\n",
        "  is_pos = preds[:, 1] > preds[:, 0]\n",
        "  is_pos_int = is_pos.astype(np.int32)\n",
        "  correct = is_pos_int == y\n",
        "  correct_float = correct.astype(np.float32)\n",
        "  sum_weights = np.sum(y_weights)\n",
        "  weighted_correct_float = correct_float * y_weights\n",
        "  weighted_num_correct = np.sum(weighted_correct_float)\n",
        "  accuracy = weighted_num_correct / sum_weights\n",
        "\n",
        "  return accuracy, weighted_num_correct, sum_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbsm9N6XKxqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(generator, model):\n",
        "\n",
        "  accuracy = 0\n",
        "  total_num_correct = 0\n",
        "  total_num_pred = 0\n",
        "\n",
        "  for batch in generator:\n",
        "    inputs = batch[0]\n",
        "    targets = batch[1]\n",
        "    example_weight = batch[2]\n",
        "\n",
        "    pred = model(inputs)\n",
        "\n",
        "    batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, \n",
        "                                                                         targets, \n",
        "                                                                         example_weight)\n",
        "    total_num_correct += batch_num_correct\n",
        "    total_num_pred += batch_num_pred\n",
        "\n",
        "  accuracy = total_num_correct / total_num_pred\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTF33D4GNgue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2e9e1bf7-da70-434f-a1fc-c49676d0a4ac"
      },
      "source": [
        "model = training_loop.eval_model\n",
        "accuracy = test_model(test_generator(16), model)\n",
        "\n",
        "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of your model on the validation set is 0.9940\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9jKxFzPbxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(sentence):\n",
        "  inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
        "  inputs = inputs[None, :]\n",
        "  preds_probs = model(inputs)\n",
        "  preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "  \n",
        "  sentiment = \"negative\"\n",
        "  if preds == 1:\n",
        "    sentiment = \"positive\"\n",
        "\n",
        "  return preds, sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpRMZHkdRGEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "b4584386-57f4-4b7d-e33a-487e9c56d648"
      },
      "source": [
        "# try a positive sentence\n",
        "sentence = \"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
        "\n",
        "print()\n",
        "# try a negative sentence\n",
        "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment of the sentence \n",
            "***\n",
            "\"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
            "***\n",
            "is positive.\n",
            "\n",
            "The sentiment of the sentence \n",
            "***\n",
            "\"I hated my day, it was the worst, I'm so sad.\"\n",
            "***\n",
            "is negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP5G_wSVRzr8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "7d7538fd-a32c-4c63-8409-c8ab2e89ae78"
      },
      "source": [
        "# try a positive sentence\n",
        "sentence = \"It's such a nice day\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
        "\n",
        "print()\n",
        "# try a negative sentence\n",
        "sentence = \"That movie was garbage\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment of the sentence \n",
            "***\n",
            "\"It's such a nice day\"\n",
            "***\n",
            "is positive.\n",
            "\n",
            "The sentiment of the sentence \n",
            "***\n",
            "\"That movie was garbage\"\n",
            "***\n",
            "is negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4BR50o_SS5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}